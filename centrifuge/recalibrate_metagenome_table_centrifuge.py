#!/usr/bin/env python

import argparse
import os
import sys
import pickle
import copy
import subprocess
import numpy as np
import pandas as pd
import warnings
import ete3
warnings.filterwarnings("ignore", message="numpy.dtype size changed")  # some compatibility issue with numpy

'''the goal of this script is to take a metagenome table and recalibrate based off of the average size per genus. The script takes the output file from centrifuge then runs through process_centrifuge.py (note the other script takes the output from malt/megan) '''

'''
USAGE EXAMPLE
./recalibrate_metagenome_table -gen_dict genome_size_dict -meta metagenome_table -host total_read_count.txt

Assumes 300bp of total reads (mapping of the paired-end reads****
'''

parser = argparse.ArgumentParser(description='recalibrate: Software for computing the read coverage per genus from a metagenome.', usage='./%(prog)s' + ' -h (help)')

# parser.add_argument('-gen_dict', '--genus_dictionary', type=str, required = True, help = "Full path to pickled dictionary of genus to genus average size mapping generated by calc_genus_average.py")

parser.add_argument('-meta', '--metagenome', type=str, required=True, help="Exported metagenome table from centrifuge")

parser.add_argument('-host', '--host_dir', type=str, help="Directory for all bam files to which host was mapped.")

parser.add_argument('-read_len', '--read_length', type=float, default=300)

parser.add_argument('-org', '--organism', type=str, default="bacteria")

parser.add_argument('-min_recal', '--minimum_recalibration', type=float, default=3870000)
# should be 3870000 for bacteria, 8970000 for fungi and 37000000 for oomycetes

parser.add_argument('-resize', '--resize_missing', type=float, default=3870000)

params = parser.parse_args()

#print("Running Metagenome Recalibrator")


def calculate_percent_mapped(sample):
    '''this function calculates the ratio of reads mapped to TAIR10 vs unmapped'''
    sample_map = float(subprocess.check_output(['/usr/bin/samtools', 'view', '-c', sample + '.bam']))
    sample_unmap = float(subprocess.check_output(['/usr/bin/samtools', 'view', '-c', sample + 'unmapped.bam']))
    sample_total = sample_unmap + sample_map
    return sample_map, sample_unmap, sample_total


def correct_depth(path):
    '''before I was using the bam files to calculate average coverage but instead Ill use the depth function which is faster. The very last line of the depth file is the average depth for the nuclear portion'''
    all_depth = {}
    for root, dirs, fnames in os.walk(path):
        for fname in fnames:
            if "depth" in fname:
                # sample = desired_file.replace(".depth", "")
                sample = fname
                #print(sample)
                error_depth = []
                depth = subprocess.check_output(['tail', '-1', sample])
                all_depth[sample.strip(".depth")] = float(depth.strip())
                # calculate_percent_mapped(path + "/"+sample)
    return all_depth


'''def findfiles(path):
this deprecated function calculates percent mapped for all bam files and can take some time
    all_bam = {}
    for root, dirs, fnames in os.walk(path):
        for fname in fnames:
            if "unmapped.bam" in fname:
                unmapped_file = fname
                sample = fname.replace("unmapped", "").replace(".bam", "")
                error_bam = []
                try:
                    all_bam[sample] = calculate_percent_mapped(path + sample)
                except subprocess.CalledProcessError:
                    error_bam.append(sample)
    return all_bam
'''


def gather_tree_family_genus(genus_dict, resize):
    '''this function gathers the newick tree from MEGAN genera and gives all genera belonging to a specific family and the family average for the genome size'''

    megan_tree = ete3.Tree("/ebio/abt6_projects9/metagenomic_controlled/Programs/metagenomics_pipeline/data/megan_genus_tree_10_2_2018.tre", format=1)

    # now iterate through every leaf and indicate its parent node (the leaves are all genera)
    genus_family_map = {}

    for leaf in megan_tree.get_leaves():
        genus_family_map[leaf.name] = leaf.get_ancestors()[0].name

    family_average = {value: [] for value in set(genus_family_map.values())}

    for key in genus_family_map:
        try:
            size = genus_dict[key][0]
            fam = genus_family_map[key]
            family_average[fam].append(size)
        except KeyError:
            fam = genus_family_map[key]
            if len(family_average[fam]) > 0:
                # print(family_average[fam])
                pass
            if len(family_average[fam]) == 0:
                # if there is no representation for family give genome size of assigned parameter resize
                # pass
                family_average[fam].append(49.99999)

    family_final = {}

    for key in family_average:
        if len([rec for rec in family_average[key] if rec != 49.99999]) > 0:
            fam_intermediate = [rec for rec in family_average[key] if rec != 49.99999]
            family_final[key] = [np.mean(fam_intermediate), 0]
        else:
            '''Assign resize value'''
            family_final[key] = [resize, 0]

    return family_final, genus_family_map


def convert_read_coverage(meta_phy, phy_dict, min_recal, resize):
    '''This function takes the data frame of genome sizes and converts the number to actual basepairs. If the genome is very small then it is resized'''
    tot_families = 0
    unidentified_families = 0
    meta_phy_new = copy.deepcopy(meta_phy)
    for phy in meta_phy.index:
        tot_families = tot_families + 1
        try:
            # resize to in bp
            size = phy_dict[phy][0] * 1000000
            # if genome is too small (poor assembly?)
            if size < 1000000:
                size = min_recal

            # correct to numpber of bp. Current is 2 x 150bp so 300bp
            reads = meta_phy.loc[phy] * 300.0

            # calculate coverage
            coverage = reads / size
            meta_phy_new.loc[phy] = coverage

        # But if the family is not in the genus database we first want to see if we can get an average for the order
        except KeyError:
            unidentified_families = unidentified_families + 1
            print("The family " + phy + " is not in the genus database")
            try:
                reads = meta_phy.loc[phy] * 300.0
                coverage = reads / resize
                meta_phy_new.loc[phy] = coverage
            except KeyError:
                print(phy + "is not included")
    return meta_phy_new, tot_families, unidentified_families


def convert_per_plant(meta_corrected, all_depth, genus_dict):
    '''output coverage per genome microbe divided by coverage per genome of A. thaliana'''
    athal_cov = {}

    # there have been problems with some bam files. Limit to those bam files that are fine.
    meta_corrected_new = {}  # copy.deepcopy(meta_corrected)  # [list(all_bam.keys())]

    for rec in meta_corrected.keys():
        new = rec.split("/")[-1].split(".R1.fq.report")[0]
        # print(rec)
        meta_corrected_new[new] = meta_corrected[rec] / all_depth[new]

    # athal = genus_dict["Arabidopsis"][0]
    '''for key in list(all_depth.keys()):
        bp_cov = all_depth[key]#read_len * all_depth[key][0] / float(athal * 1000000)
        # athal_cov[key] = bp_cov
        try:
            meta_corrected_new[key] = meta_corrected_new[key] / bp_cov
        except KeyError:
            print("Issue with " + key)
            pass
    '''
    return pd.DataFrame.from_dict(meta_corrected_new)


# if __name__ == '__main__':
# params = parser.parse_args()
# genus_size = params.genus_dictionary


# This genus_size is a dictionary of genera sizes
genus_size = "/ebio/abt6_projects9/metagenomic_controlled/Programs/metagenomics_pipeline/data/genus_dict.pck"

# This is the metagenome that is being recalibrated
metagenome = params.metagenome

# We don't really use this parameter. It should be 300bp for most studies on Hiseq3000
read_len = params.read_length

# host_reads is the directory of the host reads that were mapped. This will be used to calculate coverage of host reads
host_reads = params.host_dir

# Organism is whether we are looking at
organism = params.organism

# The recalibration for the minimum
min_recal = params.minimum_recalibration

# Resize
resize = params.resize_missing

# host_reads='/ebio/abt6_projects9/metagenomic_controlled/data/processed_reads/swedish_samples/'

# Read in the metagenome
metagenome_data = pd.read_csv(metagenome, error_bad_lines=False, sep='\t', header=0, index_col=0)

# Load the genus size dictionary
genus_dict = pickle.load(open(genus_size, 'rb'))

# the meta_family_group dataframe is just the metagenome dataframe after removal of unidentified genera ****CHECK
meta_family_group = metagenome_data.loc[[ind for ind in metagenome_data.index if ind != "None"]]
# meta_genus = metagenome_data

# Now here we take the genus_dict and calculate the average genome size in a family or genus.
family_final, genus_family_dict = gather_tree_family_genus(genus_dict, resize)

# Now we generate a table with everything converted to coverage per genome
converted = convert_read_coverage(meta_family_group, family_final, min_recal, resize)
meta_family_correct = converted[0]
family_unaccounted = converted[2] / converted[1]

# meta_corrected = convert_read_coverage(meta_genus, genus_dict)

# This goes through the output of samtools depth and gets average genome coverage which is the last line of the output
all_depth = correct_depth(host_reads)

# This converts meta table to read coverage per genome per athal coverage
meta_family_per_plant = convert_per_plant(meta_family_correct, all_depth, genus_dict)
# meta_corrected_per_plant = convert_per_plant(meta_corrected, all_bam, genus_dict)

# Now we need to output the coveted corrected table
meta_family_per_plant.to_csv("meta_family_corrected_per_plant_" + organism + ".csv", header=True, index=True)
# meta_corrected_per_plant.to_csv("meta_genus_corrected_per_plant.csv", header=True, index=True)

print(organism, str(family_unaccounted))
